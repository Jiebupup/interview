分布式系统：一组电脑透过网络相互连接传递消息与通信并协调它们的行为。组件之间彼此进行交互以实现一个共同的目标。把需要进行大量计算的工程数据分割成小块，由多台计算机分别计算，再上传运算结果后，将结果统一合并得出数据结论的科学。
集群：每个节点执行相同的任务。

1.分布式锁
在单机场景下，可以使用语言的内置锁来实现进程同步。但是在分布式场景下，需要同步的进程可能位于不同的节点上，那么就需要使用分布式锁。
阻塞锁通常使用互斥量来实现。

数据库唯一索引：创建一张锁表，当需要锁住某个方法或资源时，我们就在该表中增加一条记录，想要释放锁的时候就删除这条记录，用这个记录是否存在来判断是否存于锁定状态。
问题：
锁没有失效时间，解锁失败的话其它进程无法再获得该锁。或因服务宕机导致的锁无法释放，从而产生死锁问题。
只能是非阻塞锁，插入失败直接就报错了，无法重试；
不可重入，已经获得锁的进程也必须重新获取锁。

Redis 的 SETNX 指令（set if not exist）：指令插入一个键值对，如果 key 已经存在，那么会返回 False，否则插入成功并返回 True。
SETNX 指令和数据库的唯一索引类似，保证了只存在一个 key 的键值对，那么可以用一个 key 的键值对是否存在来判断是否存于锁定状态。
EXPIRE 指令可以为一个键值对设置一个过期时间，从而避免了数据库唯一索引实现方式中释放锁失败的问题。

释放锁就是删除 key，但是一般可以用 Lua 脚本删除，判断 value 一样才删除。可以避免释放另一个 client 创建的锁。
缺点：如果是普通的 Redis 的单实例，那就是单点故障。或者是 Redis 的普通主从，那 Redis 的主从异步复制，如果主节点挂了，key 就没有了，key 还没同步到从节点，此时从节点切换为主节点，别人就可以 set key，从而拿到锁。

Redis 的 RedLock 算法：使用了多个 Redis 实例构成集群来实现分布式锁，这是为了保证在发生单点故障时仍然可用。
尝试从 N 个互相独立 Redis 实例获取锁；
计算获取锁消耗的时间，只有当这个时间小于锁的过期时间，并且从大多数（N/2+1）实例上获取了锁，那么就认为锁获取成功了；
如果锁获取失败，就到每个实例上释放锁。

特性：
安全特性：互斥访问，即永远只有一个 client 能拿到锁。
避免死锁：最终 client 都可能拿到锁，不会出现死锁的情况，即使原本锁住某资源的 client crash 了或者出现了网络分区。
容错性：只要大部分 Redis 节点存活就可以正常提供服务。

失败重试：
如果一个 client 申请锁失败了，那么它需要稍等一会在重试避免多个 client 同时申请锁的情况，最好的情况是一个 client 需要几乎同时向 5 个 master 发起锁申请。
另外就是如果 client 申请锁失败了它需要尽快在它曾经申请到锁的 master 上执行 unlock 操作，便于其他 client 获得这把锁，避免这些锁过期造成的时间浪费。如果这时候网络分区使得 client 无法联系上这些 master，那么这种浪费就得付出代价了。

性能、崩溃恢复和 fsync：
如果没有持久化机制，client 从 5 个 master 中的 3 个处获得了锁，然后其中一个重启了，这时整个环境中又出现了 3 个 master 可供另一个 client 申请同一把锁！违反了互斥性。
如果我们开启了 AOF 持久化那么情况会稍微好转一些，因为 Redis 的过期机制是语义层面实现的，所以在 server 挂了的时候时间依旧在流逝，重启之后锁状态不会受到污染。
但是考虑断电之后呢，AOF 部分命令没来得及刷回磁盘直接丢失了，除非我们配置刷回策略为 fsnyc = always，但这会损伤性能。
解决这个问题的方法是，当一个节点重启之后，我们规定在 max TTL 期间它是不可用的，这样它就不会干扰原本已经申请到的锁，等到它 crash 前的那部分锁都过期了，环境不存在历史锁了，那么再把这个节点加进来正常工作。

Redlock 实在不是一个好的选择，对于需求性能的分布式锁应用它太重了且成本高，对于需求正确性的应用来说它不够安全。
如果你的应用只需要高性能的分布式锁不要求多高的正确性，那么单节点 Redis 够了。如果你的应用想要保住正确性，那么不建议 Redlock，建议使用一个合适的一致性协调系统，例如 Zookeeper，且保证存在 fencing token。

ZooKeeper 的有序节点：创建临时序列节点来实现分布式锁，适用于顺序执行的程序。
有序节点：会在节点名的后面加一个数字后缀，并且是有序的。
为一个节点注册监听器，在节点状态发生改变时，会给客户端发送消息。
实现：
创建一个锁目录 /lock；
当一个客户端需要获取锁时，在 /lock 下创建临时的且有序的子节点；
客户端获取 /lock 下的子节点列表，判断自己创建的子节点是否为当前子节点列表中序号最小的子节点，如果是则认为获得锁；否则监听自己的前一个子节点，获得子节点的变更通知后重复此步骤直至获得锁；
执行业务代码，完成后，删除对应的子节点。

会话超时：如果一个已经获得锁的会话超时了，因为创建的是临时节点，所以该会话对应的临时节点会被删除，其它会话就可以获得锁了。不会出现数据库的唯一索引实现的分布式锁释放锁失败问题。
羊群效应：一个节点未获得锁，只需要监听自己的前一个子节点，这是因为如果监听所有的子节点，那么任意一个子节点状态改变，其它所有子节点都会收到通知（羊群效应），而我们只希望它的后一个子节点收到通知。

Redis 分布式锁和 ZooKeeper 分布式锁的对比：
Redis 分布式锁，其实需要自己不断去尝试获取锁，比较消耗性能。ZooKeeper 分布式锁，获取不到锁，注册个监听器即可，不需要不断主动尝试获取锁，性能开销较小。
如果是 Redis 获取锁的那个客户端出现 bug 挂了，那么只能等待超时时间之后才能释放锁；而 ZooKeeper 的话，因为创建的是临时 znode，只要客户端挂了，znode 就没了，此时就自动释放锁。
Redis 分布式锁麻烦，ZooKeeper 分布式锁语义清晰实现简单。
在实践中，当然是从以可靠性为主，所以首推 ZooKeeper。

2.分布式事务
指事务的操作位于不同的节点上，且属于不同的应用，需要保证事务的 ACID 特性，需要保证这些小操作要么全部成功，要么全部失败。本质上来说，分布式事务就是为了保证不同数据库的数据一致性。
分布式锁和分布式事务区别：锁问题的关键在于进程操作的互斥关系，而事务问题的关键则在于事务涉及的一系列操作需要满足 ACID 特性。

两阶段提交 2PC：也叫 XA 方案，通过引入协调者 coordinator 来协调参与者的行为，并最终决定这些参与者是否要真正执行事务。
运行过程
准备阶段：协调者询问参与者事务是否执行成功，参与者发回事务执行结果。询问可以看成一种投票，需要参与者都同意才能执行。
提交阶段：如果事务在每个参与者上都执行成功，事务协调者发送通知让参与者提交事务；否则，协调者发送通知让参与者回滚事务。
需要注意的是，在准备阶段，参与者执行了事务，但是还未提交。只有在提交阶段接收到协调者发来的通知后，才进行提交或者回滚。

存在的问题
同步阻塞：所有事务参与者在等待其它参与者响应的时候都处于同步阻塞状态，无法进行其它操作。
单点问题：协调者在 2PC 中起到非常大的作用，发生故障将会造成很大影响。特别是在提交阶段发生故障，所有参与者会一直同步阻塞等待，无法完成其它操作。
数据不一致：在提交阶段，如果协调者只发送了部分 commit 消息，此时网络发生异常，那么只有部分参与者接收到 commit 消息，也就是说只有部分参与者提交了事务，使得系统数据不一致。
太过保守：任意一个节点失败就会导致整个事务失败，没有完善的容错机制。

这种分布式事务方案，比较适合单块应用里，跨多个库的分布式事务，而且因为严重依赖于数据库层面来搞定复杂的事务，效率很低，绝对不适合高并发的场景。如果要玩儿，那么基于 Spring + JTA 就可以搞定。
这个方案，我们很少用，一般来说某个系统内部如果出现跨多个库的这么一个操作，是不合规的。现在微服务，一个大的系统分成几十个甚至几百个服务。一般来说，我们的规定和规范，是要求每个服务只能操作自己对应的一个数据库。
如果你要操作别人的服务的库，你必须是通过调用别的服务的接口来实现，绝对不允许交叉访问别人的数据库。

三阶段提交 3PC：是两阶段提交协议的改进版本。它通过超时机制解决了阻塞的问题，并且在准备阶段和提交阶段之间多了询问阶段。
询问阶段：协调者询问参与者是否可以完成指令，协调者只需要回答是还是不是，而不需要做真正的操作，这个阶段超时导致中止

2PC 和 3PC 比较：
询问阶段可以确保尽可能早的发现无法执行操作而需要中止的行为，但是它并不能发现所有的这种行为，只会减少这种情况的发生。
在准备阶段以后，协调者和参与者执行的任务中都增加了超时，一旦超时，协调者和参与者都继续提交事务，默认为成功，这也是根据概率统计上超时后默认成功的正确性最大。
但是一旦发生超时，系统仍然会发生不一致，只不过这种情况很少见罢了，好处就是至少不会阻塞和永远锁定资源。

TCC：try、confirm、cancel，采用的补偿机制，一般来说跟钱相关的，保证强一致性。
try 阶段：对各个服务的资源做检测以及对资源进行锁定或者预留。
confirm 阶段：在各个服务中执行实际的操作。
cancel 阶段：如果任何一个服务的业务方法执行出错，那么这里就需要进行补偿，就是执行已经执行成功的业务逻辑的回滚操作（把那些执行成功的回滚）。

这个事务回滚实际上是严重依赖于你自己写代码来回滚和补偿了，会造成补偿代码巨大，非常之恶心。
而且最好是你的各个业务执行的时间都比较短。
业务代码是很难维护的。

本地消息表：本地消息表与业务数据表处于同一个数据库中，这样就能利用本地事务来保证在对这两个表的操作满足事务特性，并且使用了消息队列来保证最终一致性。
在分布式事务操作的一方完成写业务数据的操作之后向本地消息表发送一个消息，本地事务能保证这个消息一定会被写入本地消息表中。
之后将本地消息表中的消息转发到消息队列中，如果转发成功则将消息从本地消息表中删除，否则继续重新转发。
在分布式事务操作的另一方从消息队列中读取一个消息，并执行消息中的操作。

同样严重依赖于数据库的消息表来管理事务，不适合高并发场景，难扩展。

可靠消息最终一致性方案：基于 MQ 来实现事务，RocketMQ 支持事务。
A 系统先发送一个 prepared 消息到 MQ，如果这个 prepared 消息发送失败那么就直接取消操作别执行了；
如果这个消息发送成功过了，那么接着执行本地事务，如果成功就告诉 MQ 发送确认消息，如果失败就告诉 MQ 回滚消息；
如果发送了确认消息，那么此时 B 系统会接收到确认消息，然后执行本地的事务；
MQ 会自动定时轮询所有 prepared 消息回调你的接口，问你，这个消息是不是本地事务处理失败了，所有没发送确认的消息，是继续重试还是回滚？一般来说这里你就可以查下数据库看之前本地事务是否执行，如果回滚了，那么这里也回滚吧。这个就是避免可能本地事务执行成功了，而确认消息却发送失败了。
这个方案里，要是系统 B 的事务失败了咋办？重试咯，自动不断重试直到成功，如果实在是不行，要么就是针对重要的资金类业务进行回滚，比如 B 系统本地回滚后，想办法通知系统 A 也回滚；或者是发送报警由人工来手工回滚和补偿。

优点：实现了最终一致性，不需要依赖本地数据库事务。
 
最大努力通知：
系统 A 本地事务执行完之后，发送个消息到 MQ；
这里会有个专门消费 MQ 的最大努力通知服务，这个服务会消费 MQ 然后写入数据库中记录下来，或者是放入个内存队列也可以，接着调用系统 B 的接口；
要是系统 B 执行成功就 ok 了；要是系统 B 执行失败了，那么最大努力通知服务就定时尝试重新调用系统 B，反复 N 次，最后还是不行就放弃。

3.CAP
一致性 consistency：一致性指的是多个数据副本是否能保持一致的特性，在一致性的条件下，系统在执行数据更新操作之后能够从一致性状态转移到另一个一致性状态。对系统的一个数据更新成功之后，如果所有用户都能够读取到最新的值，该系统就被认为具有强一致性。
可用性 availability：可用性指分布式系统在面对各种异常时可以提供正常服务的能力。在可用性条件下，要求系统提供的服务一直处于可用的状态，对于用户的每一个操作请求总是能够在有限的时间内返回结果。  
分区容忍性 partition tolerance：分布式系统在遇到任何网络分区故障的时候，仍然需要能对外提供一致性和可用性的服务。网络分区指分布式系统中的节点被划分为多个区域，每个区域内部可以通信，但是区域之间无法通信。

在分布式系统中，分区容忍性必不可少，可用性和一致性往往是冲突的。
CP：不能访问未同步完成的节点，也就失去了部分可用性。
AP：允许读取所有节点的数据，但是数据可能不一致。

4.BASE
基本可用 basically available：指分布式系统在出现故障的时候，保证核心可用，允许损失部分可用性。
软状态 soft state：指允许系统中的数据存在中间状态，并认为该中间状态不会影响系统整体可用性，即允许系统不同节点的数据副本之间进行同步的过程存在时延。
最终一致性 eventually consistent：最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能达到一致的状态。

BASE 理论是对 CAP 中一致性和可用性权衡的结果，它的核心思想是：即使无法做到强一致性，但每个应用都可以根据自身业务特点，采用适当的方式来使系统达到最终一致性。
ACID 要求强一致性，通常运用在传统的数据库系统上。而 BASE 要求最终一致性，通过牺牲强一致性来达到可用性，通常运用在大型分布式系统中。
在实际的分布式场景中，不同业务单元和组件对一致性的要求是不同的，因此 ACID 和 BASE 往往会结合在一起使用。

针对数据库领域，BASE 思想的主要实现是对业务数据进行拆分，让不同的数据分布在不同的机器上，以提升系统的可用性。拆分后会涉及分布式事务问题。

5.分布式一致性算法
Paxos：用于达成共识性问题，即对多个节点产生的值，该算法能保证只选出唯一一个值。
Raft：用来竞选主节点。

6.如何设计一个高并发系统？
高峰期每秒并发量近万，数据库承受不住。大部分场景是读多写少。
  
dubbo 系统拆分，将一个系统拆分为多个子系统，每个系统连一个数据库。
redis 缓存，对于高并发读，在数据库和缓存里都写一份，读的时候大量走缓存。redis 轻轻松松单机几万的并发。
MQ，对于高并发写，redis 的数据随时就被 LRU 了，数据格式还无比简单，没有事务支持，无法应对写，还得用 mysql。大量的写请求灌入 MQ 里排队，后边系统消费后慢慢写，控制在 mysql 承载范围之内。MQ 单机抗几万并发也是 ok 的。
分库分表。
读写分离，主从架构。
ElasticSearch，es 是分布式的，可以随便扩容，分布式天然就可以支撑高并发，因为可以扩容加机器来扛更高的并发。一些比较简单的查询、统计类和全文搜索类的操作，可以考虑用 es 来承载。

7.分布式计算
数据在各个计算机节点上流动，同时各个计算机节点都能以某种方式访问共享数据，最终分布式计算后的输出结果被持久化存储和输出。

Actor
并行计算模型，异步并发。
并行计算原语：一个 Actor 对接收到的消息做出响应，进行本地决策，可以创建更多的 Actor（子 Actor），或者发送更多的消息；同时准备接收下一条消息。
优点：
1）将消息收发、线程调度、处理竞争和同步的所有复杂逻辑都委托给了 Actor 框架本身，而且对应用来说是透明的，我们可以认为 Actor 只是一个实现了 Runnable 接口的对象。关注多线程并发问题时，只需要关注多个 Actor 之间的消息流即可。 
2）符合 Actor 模型的程序很容易进行测试，因为任意一个 Actor 都可以被单独进行单元测试。如果测试案例覆盖了该 Actor 所能响应的所有类型的消息，我们就可以确定该 Actor 的代码十分可靠。

缺点：
1) Actor 完全避免共享并且仅通过消息来进行交流，使得程序失去了精细化并发调控能力，所以不适合实施细粒度的并行且可能导致系统响应时延的增加。如果在 Actor 程序中引入一些并行框架，就可能会导致系统的不确定性。 
2）尽管使用 Actor 模型的程序 比使用线程和锁模型的程序更容易调试，Actor 模型仍会碰到死锁这一类的共性问题，也会碰到一些 Actor 模型独有的问题（例如信箱移溢出）。

AKKA
Akka 是一个用 Scala 编写的库，用于简化编写容错的、高可伸缩性的 Java 和 Scala 的 Actor 模型应用。
Akka 处理并发的方法基于 Actor 模型。在 Akka 里，Actor 之间通信的唯一机制就是消息传递。
好处：
AKKA 提供一种 Actor 并发模型，其粒度比线程小很多，这意味着你可以在项目中使用大量的 Actor。
Akka 提供了一套容错机制，允许在 Actor 出错时进行一些恢复或者重置操作
AKKA 不仅可以在单机上构建高并发程序，也可以在网络中构建分布式程序，并提供位置透明的 Actor 定位服务

Storm
提供的是面向连续的消息流（Stream）的一种通用的分布式计算解决框架。实时流式计算。
应用场景：日志处理和电商商品推荐
Hadoop 是强大的大数据处理系统（批处理），但是在实时计算方面不够擅长；Storm 的核心功能就是提供强大的实时处理能力，但没有涉及存储；所以 Storm 与 Hadoop 互补。

MapReduce
用于大规模数据集的并行运算
“分而治之”，Mapper 负责“分”，即把复杂的大任务分解为若干个小任务来处理，彼此之间没有依赖关系，以便可以分布到多个计算节点上实现高度的并行计算能力；Reducer 则负责对 map 阶段的结果进行汇总和输出。
这个框架充分利用了磁盘，处处存在着排序和合并。所以适合于实时性不高的离线计算。
Hadoop

Spark
Spark 使用了存储器内运算技术，能在数据尚未写入硬盘时即在存储器内分析运算。Spark 允许用户将数据加载至集群存储器，并多次对其进行查询，非常适合用于机器学习算法。内存计算框架，适合在线、离线快速的大数据处理。
