1.特征
命中率：当某个请求能够通过访问缓存而得到响应时，称为缓存命中。
最大空间：缓存通常位于内存中，内存的空间通常比磁盘空间小的多，缓存的最大空间不可能非常大。

淘汰策略：
FIFO（First In First Out）：先进先出策略，在实时性的场景下，需要经常访问最新的数据，那么就可以使用 FIFO，使得最先进入的数据（最晚的数据）被淘汰。
LRU（Least Recently Used）：最近最久未使用策略，优先淘汰最久未使用的数据，也就是上次被访问时间距离现在最久的数据。该策略可以保证内存中的数据都是热点数据，也就是经常被访问的数据，从而保证缓存命中率。
LFU（Least Frequently Used）：最不经常使用策略，优先淘汰一段时间内使用次数最少的数据。

2.LRU
基于 双向链表 + HashMap
链表尾部存储的就是最近最久未使用的节点，头部存储最新节点。
get()和put()在O(1)时间完成

3.缓存位置
浏览器：当 HTTP 响应允许进行缓存时，浏览器会将 HTML、CSS、JavaScript、图片等静态资源进行缓存。
ISP：网络服务提供商（ISP）是网络访问的第一跳，通过将数据缓存在 ISP 中能够大大提高用户的访问速度。
反向代理：反向代理位于服务器之前，请求与响应都需要经过反向代理。通过将数据缓存在反向代理，在用户请求反向代理时就可以直接使用缓存进行响应。
本地缓存：使用 Guava Cache 将数据缓存在服务器本地内存中，服务器代码可以直接读取本地内存中的缓存，速度非常快。
分布式缓存：Redis、Memcache 等分布式缓存单独部署，可以根据需求分配硬件资源。不仅如此，服务器集群都可以访问分布式缓存，而本地缓存需要在服务器集群之间进行同步，实现难度和性能开销上都非常大。
数据库缓存：MySQL 等数据库管理系统具有自己的查询缓存机制来提高查询效率。
Java 内部的缓存：Java 为了优化空间，提高字符串、基本数据类型包装类的创建效率，设计了字符串常量池及 Byte、Short、Character、Integer、Long、Boolean 这六种包装类缓冲池。
CPU 多级缓存：CPU 为了解决运算速度与主存 IO 速度不匹配的问题，引入了多级缓存结构，同时使用 MESI 等缓存一致性协议来解决多核 CPU 缓存数据一致性的问题。

4.缓存问题
缓存穿透：指的是对某个一定不存在的数据进行请求，该请求将会穿透缓存到达数据库。
解决方案：对这些不存在的数据缓存一个空数据；对这类请求进行过滤。

缓存雪崩：指的是由于数据没有被加载到缓存中，或者缓存数据在同一时间大面积失效（过期），又或者缓存服务器宕机，导致大量的请求都到达数据库。而有缓存的系统中，系统非常依赖于缓存，缓存分担了很大一部分的数据请求。当发生缓存雪崩时，数据库无法处理这么大的请求，导致数据库崩溃。
解决方案：为了防止缓存在同一时间大面积过期导致的缓存雪崩，可以通过观察用户行为，合理设置缓存过期时间来实现；为了防止缓存服务器宕机出现的缓存雪崩，可以使用分布式缓存，分布式缓存中每一个节点只缓存部分的数据，当某个节点宕机时可以保证其它节点的缓存仍然可用。也可以进行缓存预热，避免在系统刚启动不久由于还未将大量数据进行缓存而导致缓存雪崩。

缓存一致性：要求数据更新的同时缓存数据也能够实时更新。要保证缓存一致性需要付出很大的代价，缓存数据最好是那些对一致性要求不高的数据，允许缓存数据存在一些脏数据。
解决方案：在数据更新的同时立即去更新缓存；在读缓存之前先判断缓存是否是最新的，如果不是最新的先进行更新。
先淘汰缓存，再写数据库：并发读写引发数据不一致。解决方法：读请求和写请求串行化。

缓存 “无底洞” 现象：为了满足业务要求添加了大量缓存节点，但是性能不但没有好转反而下降了的现象。
解决方案：优化批量数据操作命令；减少网络通信次数；降低接入成本，使用长连接 / 连接池，NIO 等。

5.CDN
内容分发网络（Content distribution network，CDN）是一种互连的网络系统，它利用更靠近用户的服务器从而更快更可靠地将 HTML、CSS、JavaScript、音乐、图片、视频等静态资源分发给用户。
优点：
更快地将数据分发给用户
通过部署多台服务器，从而提高系统整体的带宽性能
多台服务器可以看成是一种冗余机制，从而具有高可用性

6.数据分布
哈希分布，顺序分布

一致性哈希：
Distributed Hash Table（DHT） 是一种哈希分布方式，其目的是为了克服传统哈希分布在服务器节点数量变化时大量数据迁移的问题。
虚拟节点：一致性哈希存在数据分布不均匀的问题，解决方式是通过增加虚拟节点，然后将虚拟节点映射到真实节点上。虚拟节点的数量比真实节点来得多，那么虚拟节点在哈希环上分布的均匀性就会比原来的真实节点好，从而使得数据分布也更加均匀。
