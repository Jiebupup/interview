分布式系统：一组电脑透过网络相互连接传递消息与通信并协调它们的行为。组件之间彼此进行交互以实现一个共同的目标。把需要进行大量计算的工程数据分割成小块，由多台计算机分别计算，再上传运算结果后，将结果统一合并得出数据结论的科学。
集群：每个节点执行相同的任务。

1.分布式锁
数据库唯一索引：创建一张锁表，当需要锁住某个方法或资源时，我们就在该表中增加一条记录，想要释放锁的时候就删除这条记录，用这个记录是否存在来判断是否存于锁定状态。
问题：锁没有失效时间，解锁失败的话其它进程无法再获得该锁；只能是非阻塞锁，插入失败直接就报错了，无法重试；不可重入，已经获得锁的进程也必须重新获取锁。

Redis的 SETNX 指令（set if not exist）：指令插入一个键值对，如果 Key 已经存在，那么会返回 False，否则插入成功并返回 True。
保证了只存在一个 Key 的键值对，那么可以用一个 Key 的键值对是否存在来判断是否存于锁定状态。
EXPIRE 指令可以为一个键值对设置一个过期时间，从而避免了数据库唯一索引实现方式中释放锁失败的问题。

释放锁：如果某个客户端获取到了锁，但是阻塞了很长时间才执行完，可能已经自动释放锁了，可能别的客户端已经获取到了这个锁，要是你这个时候直接删除 key 的话会有问题，所以得用随机值加上 lua 脚本来释放锁。
缺点：如果是普通的 redis 单实例，那就是单点故障。或者是 redis 普通主从，那 redis 主从异步复制，如果主节点挂了（key 就没有了），key 还没同步到从节点，此时从节点切换为主节点，别人就可以 set key，从而拿到锁。

Redis 的 RedLock 算法:使用了多个 Redis 实例构成集群 redis cluster 来实现分布式锁，这是为了保证在发生单点故障时仍然可用。
尝试从 N 个互相独立 Redis 实例获取锁；
计算获取锁消耗的时间，只有当这个时间小于锁的过期时间，并且从大多数（N / 2 + 1）实例上获取了锁，那么就认为锁获取成功了；
如果锁获取失败，就到每个实例上释放锁。

特性:
安全特性：互斥访问，即永远只有一个 client 能拿到锁
避免死锁：最终 client 都可能拿到锁，不会出现死锁的情况，即使原本锁住某资源的 client crash 了或者出现了网络分区
容错性：只要大部分 Redis 节点存活就可以正常提供服务

Redlock 实在不是一个好的选择，对于需求性能的分布式锁应用它太重了且成本高，对于需求正确性的应用来说它不够安全。
如果你的应用只需要高性能的分布式锁不要求多高的正确性，那么单节点 Redis 够了。如果你的应用想要保住正确性，那么不建议 Redlock，建议使用一个合适的一致性协调系统，例如 Zookeeper，且保证存在 fencing token。

Zookeeper 的有序节点：创建临时序列节点来实现分布式锁，适用于顺序执行的程序。
实现：
创建一个锁目录 /lock；
当一个客户端需要获取锁时，在 /lock 下创建临时的且有序的子节点；
客户端获取 /lock 下的子节点列表，判断自己创建的子节点是否为当前子节点列表中序号最小的子节点，如果是则认为获得锁；否则监听自己的前一个子节点，获得子节点的变更通知后重复此步骤直至获得锁；
执行业务代码，完成后，删除对应的子节点。

会话超时：如果一个已经获得锁的会话超时了，因为创建的是临时节点，所以该会话对应的临时节点会被删除，其它会话就可以获得锁了。
可以看到，Zookeeper 分布式锁不会出现数据库的唯一索引实现的分布式锁释放锁失败问题。

羊群效应：一个节点未获得锁，只需要监听自己的前一个子节点，这是因为如果监听所有的子节点，那么任意一个子节点状态改变，其它所有子节点都会收到通知（羊群效应），而我们只希望它的后一个子节点收到通知。

redis 分布式锁和 zk 分布式锁的对比：
redis 分布式锁，其实需要自己不断去尝试获取锁，比较消耗性能。zk 分布式锁，获取不到锁，注册个监听器即可，不需要不断主动尝试获取锁，性能开销较小。
如果是 redis 获取锁的那个客户端 出现 bug 挂了，那么只能等待超时时间之后才能释放锁；而 zk 的话，因为创建的是临时 znode，只要客户端挂了，znode 就没了，此时就自动释放锁。
redis 分布式锁麻烦，zk 分布式锁语义清晰实现简单。

2.分布式事务
指事务的操作位于不同的节点上，且属于不同的应用，分布式事务需要保证这些小操作要么全部成功，要么全部失败。本质上来说，分布式事务就是为了保证不同数据库的数据一致性。

基于 XA 的 2PC 两阶段提交：通过引入协调者（事务管理器）来协调参与者的行为，并最终决定这些参与者是否要真正执行事务。
运行过程
准备阶段：协调者询问参与者事务是否执行成功，参与者发回事务执行结果。
提交阶段：如果事务在每个参与者上都执行成功，事务协调者发送通知让参与者提交事务；否则，协调者发送通知让参与者回滚事务。

存在的问题：
同步阻塞：所有事务参与者在等待其它参与者响应的时候都处于同步阻塞状态，无法进行其它操作。
单点问题：协调者在 2PC 中起到非常大的作用，发生故障将会造成很大影响。特别是在阶段二发生故障，所有参与者会一直等待，无法完成其它操作。
数据不一致：在阶段二，如果协调者只发送了部分 Commit 消息，此时网络发生异常，那么只有部分参与者接收到 Commit 消息，也就是说只有部分参与者提交了事务，使得系统数据不一致。
太过保守：任意一个节点失败就会导致整个事务失败，没有完善的容错机制。

这种分布式事务方案，比较适合单块应用里，跨多个库的分布式事务，而且因为严重依赖于数据库层面来搞定复杂的事务，效率很低，绝对不适合高并发的场景。如果要玩儿，那么基于 Spring + JTA 就可以搞定。
一般来说某个系统内部如果出现跨多个库的这么一个操作，是不合规的。

3PC：三阶段提交协议是两阶段提交协议的改进版本。它通过超时机制解决了阻塞的问题，并且多了询问阶段（在准备阶段和提交阶段之间）。
询问阶段：协调者询问参与者是否可以完成指令，协调者只需要回答是还是不是，而不需要做真正的操作，这个阶段超时导致中止

2PC和3PC比较：
询问阶段可以确保尽可能早的发现无法执行操作而需要中止的行为，但是它并不能发现所有的这种行为，只会减少这种情况的发生。
在准备阶段以后，协调者和参与者执行的任务中都增加了超时，一旦超时，协调者和参与者都继续提交事务，默认为成功，这也是根据概率统计上超时后默认成功的正确性最大。
但是一旦发生超时，系统仍然会发生不一致，只不过这种情况很少见罢了，好处就是至少不会阻塞和永远锁定资源。

TCC（Try、Confirm、Cancel）：采用的补偿机制，但代码量巨大，一般来说用在跟钱相关的场景，而且最好是各个业务执行的时间都比较短。保证强一致性。
Try 阶段：对各个服务的资源做检测以及对资源进行锁定或者预留。
Confirm 阶段：在各个服务中执行实际的操作。
Cancel 阶段：如果任何一个服务的业务方法执行出错，那么这里就需要进行补偿，就是执行已经执行成功的业务逻辑的回滚操作。（把那些执行成功的回滚）

本地消息表：本地消息表与业务数据表处于同一个数据库中，这样就能利用本地事务来保证在对这两个表的操作满足事务特性，并且使用了消息队列来保证最终一致性。
在分布式事务操作的一方完成写业务数据的操作之后向本地消息表发送一个消息，本地事务能保证这个消息一定会被写入本地消息表中。
之后将本地消息表中的消息转发到消息队列中，如果转发成功则将消息从本地消息表中删除，否则继续重新转发。
在分布式事务操作的另一方从消息队列中读取一个消息，并执行消息中的操作。

同样严重依赖于数据库的消息表来管理事务，不适合高并发场景。

MQ 事务：RocketMQ 支持事务
A 系统先发送一个 prepared 消息到 mq，如果这个 prepared 消息发送失败那么就直接取消操作别执行了；
如果这个消息发送成功过了，那么接着执行本地事务，如果成功就告诉 mq 发送确认消息，如果失败就告诉 mq 回滚消息；
如果发送了确认消息，那么此时 B 系统会接收到确认消息，然后执行本地的事务；
mq 会自动定时轮询所有 prepared 消息回调你的接口，问你，这个消息是不是本地事务处理失败了，所有没发送确认的消息，是继续重试还是回滚？一般来说这里你就可以查下数据库看之前本地事务是否执行，如果回滚了，那么这里也回滚吧。这个就是避免可能本地事务执行成功了，而确认消息却发送失败了。
这个方案里，要是系统 B 的事务失败了咋办？重试咯，自动不断重试直到成功，如果实在是不行，要么就是针对重要的资金类业务进行回滚，比如 B 系统本地回滚后，想办法通知系统 A 也回滚；或者是发送报警由人工来手工回滚和补偿。

优点：实现了最终一致性，不需要依赖本地数据库事务。
缺点：实现难度大，主流MQ不支持。

最大努力通知：
系统 A 本地事务执行完之后，发送个消息到 MQ；
这里会有个专门消费 MQ 的最大努力通知服务，这个服务会消费 MQ 然后写入数据库中记录下来，或者是放入个内存队列也可以，接着调用系统 B 的接口；
要是系统 B 执行成功就 ok 了；要是系统 B 执行失败了，那么最大努力通知服务就定时尝试重新调用系统 B，反复 N 次，最后还是不行就放弃。

3.CAP
一致性（Consistency）：一致性指的是多个数据副本是否能保持一致的特性，在一致性的条件下，系统在执行数据更新操作之后能够从一致性状态转移到另一个一致性状态。对系统的一个数据更新成功之后，如果所有用户都能够读取到最新的值，该系统就被认为具有强一致性。
可用性（Availability）：可用性指分布式系统在面对各种异常时可以提供正常服务的能力，在可用性条件下，要求系统提供的服务一直处于可用的状态，对于用户的每一个操作请求总是能够在有限的时间内返回结果。  
分区容忍性（Partition Tolerance）：分布式系统在遇到任何网络分区故障的时候，仍然需要能对外提供一致性和可用性的服务。

在分布式系统中，分区容忍性必不可少，可用性和一致性往往是冲突的。
CP：不能访问未同步完成的节点，也就失去了部分可用性。
AP：允许读取所有节点的数据，但是数据可能不一致。

4.BASE
基本可用（Basically Available）：指分布式系统在出现故障的时候，保证核心可用，允许损失部分可用性。
软状态（Soft State）：指允许系统中的数据存在中间状态，并认为该中间状态不会影响系统整体可用性，即允许系统不同节点的数据副本之间进行同步的过程存在时延。
最终一致性（Eventually Consistent）：最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能达到一致的状态。

BASE 理论是对 CAP 中一致性和可用性权衡的结果，它的核心思想是：即使无法做到强一致性，但每个应用都可以根据自身业务特点，采用适当的方式来使系统达到最终一致性。
ACID 要求强一致性，通常运用在传统的数据库系统上。而 BASE 要求最终一致性，通过牺牲强一致性来达到可用性，通常运用在大型分布式系统中。
在实际的分布式场景中，不同业务单元和组件对一致性的要求是不同的，因此 ACID 和 BASE 往往会结合在一起使用。

针对数据库领域，BASE思想的主要实现是对业务数据进行拆分，让不同的数据分布在不同的机器上，以提升系统的可用性。拆分后会涉及分布式事务问题。

5.分布式一致性算法
Paxos：用于达成共识性问题，即对多个节点产生的值，该算法能保证只选出唯一一个值。
Raft：竞选主节点。

6.负载均衡
集群中的应用服务器（节点）通常被设计成无状态，用户可以请求任何一个节点。负载均衡器会根据集群中每个节点的负载情况，将用户请求转发到合适的节点上。
负载均衡器可以用来实现高可用以及伸缩性。
高可用：当某个节点故障时，负载均衡器会将用户请求转发到另外的节点上，从而保证所有服务持续可用；
伸缩性：根据系统整体负载情况，可以很容易地添加或移除节点。

负载均衡算法
轮询（Round Robin）：把每个请求轮流发送到每个服务器上。
该算法比较适合每个服务器的性能差不多的场景，如果有性能存在差异的情况下，那么性能较差的服务器可能无法承担过大的负载。

加权轮询（Weighted Round Robbin）：在轮询的基础上，根据服务器的性能差异，为服务器赋予一定的权值，性能高的服务器分配更高的权值。

由于每个请求的连接时间不一样，使用轮询或者加权轮询算法的话，可能会让一台服务器当前连接数过大，而另一台服务器的连接过小，造成负载不均衡。

最少连接（least Connections）：将请求发送给当前最少连接数的服务器上。

加权最少连接（Weighted Least Connection）：在最少连接的基础上，根据服务器的性能为每台服务器分配权重，再根据权重计算出每台服务器能处理的连接数。

随机算法（Random）：和轮询算法类似，该算法比较适合服务器性能差不多的场景。

源地址哈希法 (IP Hash)：源地址哈希通过对客户端 IP 计算哈希值之后，再对服务器数量取模得到目标服务器的序号。
可以保证同一 IP 的客户端的请求会转发到同一台服务器上，用来实现会话粘滞（Sticky Session）

转发实现
HTTP 重定向：HTTP 重定向负载均衡服务器使用某种负载均衡算法计算得到服务器的 IP 地址之后，将该地址写入 HTTP 重定向报文中，状态码为 302。客户端收到重定向报文之后，需要重新向服务器发起请求。
缺点：
需要两次请求，因此访问延迟比较高；HTTP 负载均衡器处理能力有限，会限制集群的规模。
实际场景中很少使用。

DNS 域名解析：在 DNS 解析域名的同时使用负载均衡算法计算服务器 IP 地址。
优点：DNS 能够根据地理位置进行域名解析，返回离用户最近的服务器 IP 地址。
缺点：由于 DNS 具有多级结构，每一级的域名记录都可能被缓存，当下线一台服务器需要修改 DNS 记录时，需要过很长一段时间才能生效。 
大型网站基本使用了 DNS 作为第一级负载均衡手段，然后在内部使用其它方式做第二级负载均衡。也就是说，域名解析的结果为内部的负载均衡服务器 IP 地址。

反向代理服务器：反向代理服务器位于源服务器前面，用户的请求需要先经过反向代理服务器才能到达源服务器。反向代理可以用来进行缓存、日志记录等，同时也可以用来做为负载均衡服务器。
在这种负载均衡转发方式下，客户端不直接请求源服务器，因此源服务器不需要外部 IP 地址，而反向代理需要配置内部和外部两套 IP 地址。
优点：与其它功能集成在一起，部署简单。
缺点：所有请求和响应都需要经过反向代理服务器，它可能会成为性能瓶颈。

网络层：在操作系统内核进程获取网络数据包，根据负载均衡算法计算源服务器的 IP 地址，并修改请求数据包的目的 IP 地址，最后进行转发。
源服务器返回的响应也需要经过负载均衡服务器，通常是让负载均衡服务器同时作为集群的网关服务器来实现。
优点：在内核进程中进行处理，性能比较高。  
缺点：和反向代理一样，所有的请求和响应都经过负载均衡服务器，会成为性能瓶颈。

链路层：在链路层根据负载均衡算法计算源服务器的 MAC 地址，并修改请求数据包的目的 MAC 地址，并进行转发。
通过配置源服务器的虚拟 IP 地址和负载均衡服务器的 IP 地址一致，从而不需要修改 IP 地址就可以进行转发。也正因为 IP 地址一样，所以源服务器的响应不需要转发回负载均衡服务器，可以直接转发给客户端，避免了负载均衡服务器的成为瓶颈。
这是一种三角传输模式，被称为直接路由。对于提供下载和视频服务的网站来说，直接路由避免了大量的网络传输数据经过负载均衡服务器。
这是目前大型网站使用最广负载均衡转发方式，在 Linux 平台可以使用的负载均衡服务器为 LVS（Linux Virtual Server）。

7.如何设计一个高并发系统？
高峰期每秒并发量近万，数据库承受不住。大部分场景是读多写少。
  
dubbo系统拆分，将一个系统拆分为多个子系统，每个系统连一个数据库。
redis缓存，对于高并发读，在数据库和缓存里都写一份，读的时候大量走缓存。redis 轻轻松松单机几万的并发。
MQ，对于高并发写，redis 的数据随时就被 LRU 了，数据格式还无比简单，没有事务支持，无法应对写，还得用 mysql。大量的写请求灌入 MQ 里排队，后边系统消费后慢慢写，控制在 mysql 承载范围之内。MQ 单机抗几万并发也是 ok 的。
分库分表。
读写分离，主从架构。
ElasticSearch，es 是分布式的，可以随便扩容，分布式天然就可以支撑高并发，因为可以扩容加机器来扛更高的并发。一些比较简单的查询、统计类和全文搜索类的操作，可以考虑用 es 来承载。

8.分布式计算
数据在各个计算机节点上流动，同时各个计算机节点都能以某种方式访问共享数据，最终分布式计算后的输出结果被持久化存储和输出。

Actor
并行计算模型，异步并发。
并行计算原语：一个Actor对接收到的消息做出响应，进行本地决策，可以创建更多的Actor（子Actor），或者发送更多的消息；同时准备接收下一条消息。
优点：
1）将消息收发、线程调度、处理竞争和同步的所有复杂逻辑都委托给了Actor框架本身，而且对应用来说是透明的，我们可以认为Actor只是一个实现了Runnable接口的对象。关注多线程并发问题时，只需要关注多个Actor之间的消息流即可。 
2）符合Actor模型的程序很容易进行测试，因为任意一个Actor都可以被单独进行单元测试。如果测试案例覆盖了该Actor所能响应的所有类型的消息，我们就可以确定该Actor的代码十分可靠。

缺点：
1) Actor完全避免共享并且仅通过消息来进行交流，使得程序失去了精细化并发调控能力，所以不适合实施细粒度的并行且可能导致系统响应时延的增加。如果在Actor程序中引入一些并行框架，就可能会导致系统的不确定性。 
2）尽管使用Actor模型的程序 比使用线程和锁模型的程序更容易调试，Actor模型仍会碰到死锁这一类的共性问题，也会碰到一些Actor模型独有的问题（例如信箱移溢出）。

AKKA
Akka 是一个用 Scala 编写的库，用于简化编写容错的、高可伸缩性的 Java 和 Scala 的 Actor 模型应用。
Akka处理并发的方法基于Actor模型。在Akka里，Actor之间通信的唯一机制就是消息传递。
好处：
AKKA提供一种Actor并发模型，其粒度比线程小很多，这意味着你可以在项目中使用大量的Actor。
Akka提供了一套容错机制，允许在Actor出错时进行一些恢复或者重置操作
AKKA不仅可以在单机上构建高并发程序，也可以在网络中构建分布式程序，并提供位置透明的Actor定位服务

Storm
提供的是面向连续的消息流（Stream）的一种通用的分布式计算解决框架。实时流式计算。
应用场景：日志处理和电商商品推荐
Hadoop 是强大的大数据处理系统（批处理），但是在实时计算方面不够擅长；Storm的核心功能就是提供强大的实时处理能力，但没有涉及存储；所以 Storm 与 Hadoop 互补。

MapReduce
用于大规模数据集的并行运算
“分而治之”，Mapper负责“分”，即把复杂的大任务分解为若干个小任务来处理，彼此之间没有依赖关系，以便可以分布到多个计算节点上实现高度的并行计算能力；Reducer则负责对map阶段的结果进行汇总和输出。
这个框架充分利用了磁盘，处处存在着排序和合并。所以适合于实时性不高的离线计算。
Hadoop

Spark
Spark使用了存储器内运算技术，能在数据尚未写入硬盘时即在存储器内分析运算。Spark允许用户将数据加载至集群存储器，并多次对其进行查询，非常适合用于机器学习算法。内存计算框架，适合在线、离线快速的大数据处理。
