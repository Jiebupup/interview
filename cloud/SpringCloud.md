## 简介

Spring Cloud 是一个规范，Spring Cloud Netflix 和 Spring Cloud Alibaba 都是其实现。目前使用 Spring Cloud 的第一选择是 Spring Cloud Netflix。

**Spring Cloud Netflix** 

包括 Ribbon、Feign、Eureka（不再开源）、Hystrix 等。

**Spring Cloud Alibaba** 

包括 Sentinel、Dubbo、Nacos、RocketMQ、Seata 等。



现在开发微服务也可以直接使用 Kubernetes。



## 1.微服务架构

描述了将软件应用程序设计为若干个可独立部署的服务套件的特定方法。分布式系统。

**服务**

微服务架构风格是一种将单个应用程序开发为一套小型服务的方法，每个小型服务都在自己的进程中运行，并以轻量级机制（通常是 HTTP 资源 API）进行通信。

这些服务围绕业务功能构建，可通过全自动部署机制来独立部署，可以独立扩展和监控，有自己的资源，每个服务还提供了一个牢固的模块边界，服务单元之间松耦合。

这些服务共用一个最小型的集中式管理，它们可以使用不同的编程语言编写，并使用不同的数据存储技术，可以由不同的团队管理。

**单体式**

变更周期被捆绑在一起，即使只是对应用程序的一小部分进行了更改，也需要重建和部署整个单体应用。

随着时间的推移，通常很难保持良好的模块化结构，也更难以保持应该只影响该模块中的一个模块的更改。

对系统进行扩展时，不得不扩展整个应用系统，而不能仅扩展该系统中需要更多资源的那些部分。

应用太复杂，修正 bug 和正确的添加新功能变的非常困难，并且很耗时，降低开发速度，不利于持续性开发。在不同模块发生资源冲突时，扩展将会非常困难。

可靠性差。因为所有模块都运行在一个进程中，任何一个模块中的一个 bug 将会有可能弄垮整个进程。除此之外，因为所有应用实例都是唯一的，这个 bug 将会影响到整个应用的可靠性。

#### **单体式迁移到微服务**

不要大规模重写代码，相反应该采取逐步迁移单体式应用的策略，通过逐步生成微服务新应用，与旧的单体式应用集成，随着时间推移，单体式应用在整个架构中比例逐渐下降直到消失或者成为微服务架构一部分。

**做法**

1. 停止挖掘，停止让单体式应用继续变大，将新功能开发成独立微服务。新增请求路由器发送请求，新增胶水代码 glue code 数据整合和容灾。

2. 将前端和后端分离。使得应用两部分开发、部署和扩展各自独立。允许表现层开发者在用户界面上快速选择，进行 A/B 测试。使得一些远程 API 可以被微服务调用。
3. 将现存模块抽取变成微服务。将资源消耗大户先抽取。查找现有粗粒度边界来决定哪个模块应该被抽取，使得移植工作更容易和简单。抽取模块第一步就是定义好模块和单体应用之间粗粒度接口，第二步就是将模块转换成独立服务。

#### **微服务的优点**

- 通过分解巨大单体式应用为多个服务方法解决了复杂性问题，每个服务都有清楚的边界。模块化，单个服务很容易开发、理解和维护。降低耦合。可复用。

- 每个服务都可以有专门开发团队来开发。开发者可以自由选择开发技术，提供 API 服务。屏蔽与自身业务无关技术细节，显著减少代码冲突。
- 每个微服务独立部署。开发者不再需要协调其它服务部署对本服务的影响。这种改变可以加快部署速度。
- 每个服务独立扩展。你可以根据每个服务的规模来部署满足需求的规模。甚至于，你可以使用更适合于服务资源需求的硬件。

- 数据隔离，避免不同业务模块间的数据耦合。

#### **微服务的缺点**

- 提高复杂性。测试和部署复杂，
- 分区的数据库架构一致性问题。数据拆分和迁移问题。
- 稳定性问题，应用的改变将会波及多个服务。一个服务故障后，依赖他的服务受到牵连也发生故障。
- 服务调用关系错综复杂，链路过长，问题难定位。



## 2.如何解决微服务的问题

稳定性：
雪崩效应：微服务化后，服务变多，调用链路变长，如果一个调用链上某个服务节点出问题，很可能引发整个调用链路崩溃，也就是所谓的雪崩效应。
解决方案：在服务间加熔断（Hystrix），或者在服务内（JVM 内）线程隔离。

如何应对突发流量对服务的巨大压力？
在网关层（Zuul，Gateway，Nginx 等）做限流，就按照一定策略抛弃超出阈值的访问请求。
全局限流和 IP 限流：应对突发流量，避免系统被压垮。
userID 限流和 IP 限流：防刷，防止机器人脚本等频繁调用服务。

数据冗余：可以结合熔断一起使用，作为熔断的后备方案。
部署隔离、数据隔离和业务隔离。

持续集成测试、性能测试、监控、CDN、网络带宽、避免单点问题、灰度发布系统。

服务降级：
互联网分布式系统中，经常会有一些异常状况导致服务器压力剧增，服务降级为了保证系统核心功能的稳定性和可用性。

关闭次要功能
降低一致性之读降级：对于读一致性要求不高的场景。在服务和数据库压力过大时，可以不读数据库，降级为只读缓存数据。以这种方式来减小数据库压力，提高服务的吞吐量。
降低一致性之写入降级：在服务压力过大时，可以将同步调用改为异步消息队列方式，来减小服务压力并提高吞吐量。既然把同步改成了异步也就意味着降低了数据一致性，保证数据最终一致即可。
屏蔽写入：很多高并发场景下，查询请求都会走缓存，这时数据库的压力主要是写入压力。所以对于某些不重要的服务，在服务和数据库压力过大时，可以关闭写入功能，只保留查询功能。这样可以明显减小数据库压力。
数据冗余：服务调用者可以冗余它所依赖服务的数据。当依赖的服务故障时，服务调用者可以直接使用冗余数据。数据冗余可以结合熔断一起使用，实现自动降级，表现在熔断后要返回的数据从冗余里取。
熔断和后备：熔断是一种自动降级手段。当服务不可用时，用来避免连锁故障，雪崩效应。发生在服务调用的时候，在调用方做熔断处理。熔断的意义在于，调用方快速失败 Fail Fast，避免请求大量阻塞。并且保护被调用方。
限流：对于突发流量，我们可以通过限流来保护后端服务，网关层是最合适的限流位置。另外，考虑到用户体验问题，我们还需要相应的限流页面。对于服务层的限流，我们一般可以利用 spring AOP，以拦截器的方式做限流处理。细节：限流算法和毛刺现象。

熔断和限流等属于自动降级的范畴，由于业务过于复杂，需要参考的指标太多，自动降级实现起来难度会比较大，而且也很容易出错。所以在考虑做自动降级之前一定要充分做好评估，相应的自动降级方案也要考虑周全。
大型互联网公司基本都会有自己的降级平台，大部分降级都在平台上操作，比如手动降级开关，批量降级顺序管理，熔断阈值动态设置，限流阈值动态设置等等。

数据一致性问题：
服务化后单体系统被拆分成多个服务，各服务访问自己的数据库。而我们的一次请求操作很可能要跨多个服务，同时要操作多个数据库的数据，我们发现以前用的数据库事务不好用了。

TCC：
confirm 阶段扣减库存失败了，框架会不断重试调用库存服务直到成功（考虑性能问题，重试次数应该有限制）。cancel 过程也是一样的道理。既然需要重试，我们就要保证接口的幂等性。

try 阶段因为是 rpc 远程调用，在网络拥塞等情况下，是有可能超时的。tcc 框架会回滚 cancel 整个分布式事务，回滚完成后冻结库存请求才到达库存服务并执行，这时被冻结的库存就没办法恢复了。这种情况称之为悬挂，也就是说预留的资源后续无法处理。
解决方案：第二阶段已经执行，第一阶段就不再执行了，可以加一个分支事务记录表，如果表里有相关第二阶段的执行记录，就不再执行 try。分布式环境下，我们可以考虑对第二阶段执行记录的查询和插入加上分布式锁，确保万无一失。

RocketMQ 最终一致：
发送半消息，所有事务型消息都要经历确认过程，从而确定最终提交或回滚（抛弃消息），未被确认的消息称为半消息、预备消息或待确认消息。半消息是消息发送方发送的，如果失败，发送方自己是知道的并可以做相应处理。
半消息发送成功并响应给发送方。
执行本地事务，根据本地事务执行结果，发送提交或回滚的确认消息。
如果确认消息丢失（网络问题或者生产者故障等问题），MQ 向发送方回查执行结果。
根据上一步骤回查结果，确定提交或者回滚（抛弃消息）。

假如发送方执行完本地事务后，发送确认消息通知 MQ 提交或回滚消息时失败了（网络问题，发送方重启等情况），怎么办？
当 MQ 发现一个消息长时间处于半消息（待确认消息）的状态，MQ 会以定时任务的方式主动回查发送方并获取发送方执行结果。这样即便出现网络问题或者发送方本身的问题（重启，宕机等），MQ 通过定时任务主动回查发送方基本都能确认消息最终要提交还是回滚（抛弃）。当然出于性能和半消息堆积方面的考虑，MQ 本身也会有回查次数的限制。

如何保证消费一定成功呢？
RocketMQ 本身有 ack 机制，来保证消息能够被正常消费。如果消费失败（消息订阅方出错，宕机等原因），RocketMQ 会把消息重发回 Broker，在某个延迟时间点后（默认 10 秒后）重新投递消息。

数据迁移：
微服务化，其中一个重要意义在于数据隔离，即不同的服务对应各自的数据库表，避免不同业务模块间数据的耦合。
数据迁移过程我们要注意哪些关键点呢？第一，保证迁移后数据准确不丢失，即每条记录准确而且不丢失记录；第二，不影响用户体验（尤其是访问量高的 C 端业务需要不停机平滑迁移）；第三，保证迁移后的性能和稳定性。
业务重要程度一般或者是内部系统，数据结构不变，这种场景下可以采用挂从库，数据同步完找个访问低谷时间段，停止服务，然后将从库切成主库，再启动服务。简单省时，不过需要停服避免切主库过程数据丢失。
重要业务，并发高，数据结构改变。这种场景一般需要不停机平滑迁移。

开启双写，新老库同时写入。任何对数据库的增删改都要双写；对于更新操作，如果新库没有相关记录，先从老库查出记录更新后写入数据库；为了保证写入性能，老库写完后，可以采用消息队列异步写入新库。同时写两个库，不在一个本地事务，有可能出现数据不一致的情况，这样就需要一定的补偿机制来保证两个库数据最终一致。
将某时间戳之前的老数据迁移到新库（需要脚本程序做老数据迁移，因为数据结构变化比较大的话，从数据库层面做数据迁移就很困难了），注意：1，时间戳一定要选择开启双写后的时间点，避免部分老数据被漏掉；2，迁移过程遇到记录冲突直接忽略（因为第一步有更新操作，直接把记录拉到了新库）；迁移过程一定要记录日志，尤其是错误日志。还需要通过脚本程序检验数据，看新库数据是否准确以及有没有漏掉的数据。
数据校验没问题后，开启双读。起初新库给少部分流量，新老两库同时读取，由于时间延时问题，新老库数据可能有些不一致，所以新库读不到需要再读一遍老库。逐步将读流量切到新库，相当于灰度上线的过程。遇到问题可以及时把流量切回老库。
读流量全部切到新库后，关闭老库写入（可以在代码里加上可热配开关），只写新库。移完成，后续可以去掉双写双读相关无用代码。

老数据迁移脚本程序和检验程序可以工具化，以后再做类似的数据迁移可以复用。

全链路 APM 监控：
务化之后调用链路变长，排查性能问题可能要跨多个服务，定位问题更加困难；服务变多，服务间调用关系错综复杂，不清楚服务间的依赖和调用关系，之后的系统维护过程也会更加艰巨。
通过拓扑图我们可以清晰地看到服务与服务，服务与数据库，服务与缓存中间件的调用和依赖关系。对服务关系了如指掌之后，也可以避免服务间循依赖、循环调用的问题。
请求调用栈 Call Stack 监控。
Server Map 是 Pinpoint 另一个比较重要的功能。如上图，我们不但能清晰地看到一个请求的访问链路，而且还能看到每个节点的访问次数，为系统优化提供了有力的依据。
Pinpoint 还有 JVM 监控，堆内存，活跃线程，CPU，文件描述符等监控。



## 3.微服务数据管理问题

基于微服务的应用一般都使用 SQL 和 NoSQL 结合的数据库，也就是被称为 polyglot persistence 的方法。

polyglot persistent 架构用于存储数据有许多优势，包括松耦合服务和更佳性能和可扩展性。

分布式数据管理带来的挑战：同时保持多个服务之间数据一致性，从多个服务中获取一致性数据。

#### 事件驱动架构

解决办法是采用事件驱动架构。在这种架构中，当某件重要事情发生时，微服务会发布一个事件。

**优点**

此架构可以使得交易跨多个服务且提供最终一致性，并且可以使应用维护最终视图。

**缺点**

- 编程模式比 ACID 交易模式更加复杂，为了从应用层级失效中恢复，还需要完成补偿性交易。

- 应用必须应对不一致的数据，这是因为临时交易造成的改变是可见的。

- 当应用读取未更新的最终视图时也会遇见数据不一致问题。
- 订阅者必须检测和忽略冗余事件。

#### **原子操作 Achieving Atomicity**

事件驱动架构还会碰到数据库更新和发布事件原子性问题。

**挖掘数据库交易日志**

应用更新数据库，在数据库交易日志中产生变化，交易日志挖掘进程或者线程读这些交易日志，将日志发布给消息代理。

**优点**

是确保每次更新发布事件不依赖于 2PC。交易日志挖掘可以通过将发布事件和应用业务逻辑分离开得到简化。

**缺点**

在于交易日志对不同数据库有不同格式，甚至不同数据库版本也有不同格式；而且很难从底层交易日志更新记录转换为高层业务事件。

**使用事件源**

通过使用根本不同的事件中心方式来获得不需 2PC 的原子性，保证业务实体的一致性。 这种应用保存业务实体一系列状态改变事件，而不是存储实体现在的状态。应用可以通过重放事件来重建实体现在状态。只要业务实体发生变化，新事件就会添加到时间表中。因为保存事件是单一操作，因此肯定是原子性的。

**优点**



## 4.微服务部署策略

#### **单主机多服务实例模式**

需要提供若干台物理或者虚拟机，每台机器上运行多个服务实例。

这种模式有一些参数，一个参数代表每个服务实例由多少进程构成。另外一个参数定义同一进程组内有多少服务实例运行。

**优点**

- 在于资源利用有效性。多服务实例共享服务器和操作系统，如果进程组运行多个服务实例效率会更高。例如，多个 web 应用共享同一个 Tomcat server 和 JVM。

- 部署服务实例很快。只需将服务拷贝到主机并启动它。因为没有太多负载，启动服务很快。如果服务是自包含的进程，只需要启动就可以；否则，如果是运行在容器进程组中的某个服务实例，则需要动态部署进容器中，或者重启容器。

**缺点**

- 服务实例间很少或者没有隔离，除非每个服务实例是独立进程。如果想精确监控每个服务实例资源使用，就不能限制每个实例资源使用。因此有可能造成某个糟糕的服务实例占用了主机的所有内存或者 CPU。
- 同一进程内多服务实例没有隔离。某个糟糕服务实例很容易攻击同一进程中其它服务；更甚至于，有可能无法监控每个服务实例使用的资源情况。
- 运维团队必须知道如何部署的详细步骤。服务可以用不同语言和框架写成，因此开发团队肯定有很多需要跟运维团队沟通事项。其中复杂性增加了部署过程中出错的可能性。

#### 单主机单服务实例模式

每个主机上服务实例都是各自独立的。分为单虚拟机单实例和单容器单实例。

**单虚拟机单实例**

一般将服务打包成虚拟机映像 image。

**优点**

- 每个服务实例都是完全独立运行的，都有各自独立的 CPU 和内存而不会被其它服务占用。

- 用户可以使用成熟云架构，云服务都提供如负载均衡和扩展性等有用功能。

- 服务实施技术被自包含了。一旦服务被打包成 VM 就成为一个黑盒子。VM 的管理 API 成为部署服务的 API，部署成为一个非常简单和可靠的事情。

**缺点**

- 资源利用效率不高。每个服务实例占用整个虚机的资源，包括操作系统。而且，在一个典型的公有 IaaS 环境，虚机资源都是标准化的，有可能未被充分利用。

- 公有 IaaS 根据 VM 来收费，而不管虚机是否繁忙。例如 AWS 提供了自动扩展功能，但是对随需应用缺乏快速响应，使得用户不得不多部署虚机，从而增加了部署费用。

- 部署服务新版本比较慢。虚机镜像因为大小原因创建起来比较慢，同样原因，虚机初始化也比较慢，操作系统启动也需要时间。但是这并不一直是这样，一些轻量级虚机，例如使用 Boxfuse 创建的虚机，就比较快。

- 对于运维团队，它们负责许多客制化工作。除非使用如 Boxfuse 之类的工具，可以帮助减轻大量创建和管理虚机的工作；否则会占用大量时间从事与核心业务不太无关的工作。

**单容器单实例**

每个服务实例都运行在各自容器中。k8s 管理容器集群。

**优点**

- 容器的优点跟虚机很相似。容器和虚机之间的区别越来越模糊。

- 不一样的是，容器是一个轻量级技术。容器映像创建起来很快。因为不需要操作系统启动机制，容器启动也很快。当容器启动时，后台服务就启动了。

**缺点**

- 尽管容器架构发展迅速，但是还是不如虚机架构成熟。而且由于容器之间共享 host OS 内核因此并不像虚机那么安全。

- 容器技术将会对管理容器映像提出许多客制化需求，除非使用如 Google Container Engine 或者 Amazon EC2 Container Service，否则用户将同时需要管理容器架构以及虚机架构。

- 容器经常被部署在按照虚机收费的架构上，很显然，客户也会增加部署费用来应对负载的增长。

#### Serverless 部署

避免了前述虚拟机和容器技术的缺陷。

需要将服务打包成 ZIP 文件上载到 AWS Lambda 就可以部署。可以提供元数据，提供处理服务请求函数的名字（一个事件）。AWS Lambda 自动运行处理请求足够多的微服务，然而只根据运行时间和消耗内存量来计费。

**Lambda 函数**

无状态服务，一般通过激活 AWS 服务处理请求。例如，当映像上载到 S3 bucket 激活 Lambda 函数后，就可以在 DynamoDB 映像表中插入一个条目，给 Kinesis 流发布一条消息，触发映像处理动作。Lambda 函数也可以通过第三方 web 服务激活。

四种方法激活 Lambda 函数。

**优点**

AWS Lambda 是一种很方便部署微服务的方式。基于请求计费方式意味着用户只需要承担处理自己业务那部分的负载；另外，因为不需要了解基础架构，用户只需要开发自己的应用。

**缺点**

- 不需要用来部署长期服务

- 服务必须是无状态
- 必须用某种支持的语言完成，服务必须启动很快，否则，会因为超时被停止。

#### 微服务通讯

**同步**

REST 和 RPC。

**异步**

MQ。



## 5.服务注册发现 Eureka

Eureka 是由 Netflix 公司开源，采用的是 C/S 模式进行设计，基于 HTTP 协议和使用 Restful API 开发的服务注册与发现组件，提供了完整的服务注册和服务发现，可以和 Spring Cloud 无缝集成。其中 server 端扮演着服务注册中心的角色，主要是为 client 端提供服务注册和发现等功能，维护着 client 端的服务注册信息，同时定期心跳检测已注册的服务当不可用时将服务剔除下线，client 端可以通过 server 端获取自身所依赖服务的注册信息，从而完成服务间的调用。

负载均衡和故障转移，保证了可用性。

#### 微服务中多个服务之间调用

**维护配置**

最简单的方式，让服务 A 自己去维护一份服务 B 的配置（包含 IP 地址和端口等信息）。

**缺点**

- 随着我们调用服务数量的增加，配置文件该如何维护。

- 缺乏灵活性，如果服务 B 改变 IP 地址或者端口，服务 A 也要修改相应的文件配置。

- 还有一个就是进行服务的动态扩容或缩小不方便。 

**服务发现**

抽象出来了一个注册中心，当一个新的服务上线时，它会将自己的 IP 和端口注册到注册中心去，会对注册的服务进行定期的心跳检测，当发现服务状态异常时将其从注册中心剔除下线。服务 A 只要从注册中心中获取服务 B 的信息即可，即使当服务 B 的 IP 或者端口变更了，服务 A 也无需修改，从一定程度上解耦了服务。

充当服务发现的组件：Eureka 和 Zookeeper。

#### 服务注册中心 Eureka Server

我们在项目中引入 Eureka Server 的相关依赖，然后在启动类加上注解 @EnableEurekaServer ，就可以将其作为注册中心。

我们继续添加两个模块 service-provider ， service-consumer ，然后在启动类加上注解 @EnableEurekaClient 并指定注册中心地址为我们刚刚启动的 Eureka Server ，再次访问可以看到两个服务都已经注册进来了。

**服务注册 Register**

当 Eureka Client Provider 向 Eureka Server 注册时（REST 请求），它提供自身的元数据 metaData，比如 IP 地址、端口，运行状况指示符 URL，主页等。

发送一个 POST 请求带上当前实例信息到类 ApplicationResource 的 addInstance 方法进行服务注册。后面的操作类似。

**服务续约 Renew**

Eureka Client 会每隔 30 秒（默认情况下）发送一次心跳来续约。通过续约来告知 Eureka Server 该 Eureka Client 仍然存在，没有出现问题。

**服务剔除 Eviction**

正常情况下，如果 Eureka Server 在 90 秒没有收到 Eureka Client 的续约，它会将实例从其注册表中删除。

**服务同步 Synchronize**

两个服务提供者分别注册到了两个不同的服务注册中心上，信息分别被两个服务注册中心所维护。

此时，由于服务注册中心之间因互相注册为服务，当服务提供者发送注册请求到一个服务注册中心时，它会将该请求转发给集群中相连的其他注册中心，从而实现注册中心之间的服务同步。

通过服务同步，两个服务提供者的服务信息就可以通过这两台服务注册中心中的任意一台获取到。

**服务下线 Cancel**

Eureka Client 在程序关闭时向 Eureka Server 发送取消请求。发送请求后，Eureka Client 实例信息将从 Eureka Server 的实例注册表中删除，并把该下线事件传播出去。该下线请求不会自动完成。

**自我保护**

Eureka Server 在运行期间，会统计心跳失败的比例在 15 分钟之内是否低于 85%，如果出现低于的情况，Eureka Server 会将当前的实例注册信息保护起来，让这些实例不会过期，尽可能保护这些注册信息。

但是，在这段保护期间内实例若出现问题，那么 Eureka Client 很容易拿到实际已经不存在的服务实例，会出现调用失败的情况，所以 Eureka Client 必须要有容错机制，比如可以使用请求重试、断路器等机制。

#### 服务提供者 Eureka Client Provider

**获取注册列表信息 Fetch Registries**

Eureka Server 缓存注册列表信息，整个注册表以及每个应用程序的信息进行了压缩，压缩内容和没有压缩的内容完全相同。

Eureka Client 从 Eureka Server 获取注册表信息，并将其缓存在本地。Eureka Client 会使用该信息查找其他服务，从而进行远程调用。该注册列表信息定期（每 30 秒钟）更新一次。每次返回注册列表信息可能与 Eureka Client 的缓存信息不同，Eureka Client 自动处理。如果由于某种原因导致注册列表信息不能及时匹配，Eureka Client 则会重新获取整个注册表信息。

Eureka Client 和 Eureka Server 可以使用 JSON/XML 格式进行通讯。在默认的情况下 Eureka Client 使用压缩 JSON 格式来获取注册列表的信息。

对于访问实例的选择，Eureka 中有 Region 和 Zone 的概念，一个 Region 中可以包含多个 Zone，每个服务 Eureka Client 需要被注册到一个 Zone 中，所以每个 Eureka Client 对应一个 Region 和一个 Zone。在进行服务调用的时候，优先访问同处一个一个 Zone 中的服务提供方，若访问不到，就访问其他的 Zone。

#### 服务消费者 Eureka Client Consumer

从注册中心 获取服务列表 Fetch 和更新服务列表 Update。

#### Eureka 和 Zookeeper 的区别

Eureka 保证 AP，Zookeeper 保证 CP：

- C：数据一致性；
- A：服务可用性；
- P：服务对网络分区故障的容错性。



## 6.负载均衡 Ribbon

Ribbon 是 Netflix 公司的一个开源的负载均衡项目，是一个客户端/进程内负载均衡器，运行在消费者端。
其工作原理就是 Consumer 端获取到了所有的服务列表之后，在其内部使用负载均衡算法，进行对多个系统的调用。
Ribbon 是一个基于 HTTP 和 TCP 的客户端负载均衡器，需要和 Eureka 配合。

Nginx 和 Ribbon 的对比：
Nignx 和 Ribbon 不同的是，它是一种集中式的负载均衡器。将所有请求都集中起来，然后再进行负载均衡。
Nginx 中请求是先进入负载均衡器，而在 Ribbon 中是先在客户端进行负载均衡才进行请求的。

Ribbon 中有更多的负载均衡调度算法，其默认是使用的 RoundRobinRule 轮询策略。还有 RandomRule 随机策略和 RetryRule 重试策略。
在 Ribbon 中你还可以自定义负载均衡算法，你只需要实现 IRule 接口，然后修改配置文件或者自定义 Java Config 类。

通过 Spring Cloud Ribbon 的封装，我们在微服务架构中使用客户端负载均衡调用只需要如下两步：
服务提供者只需要启动多个服务实例并且注册到一个注册中心或是多个相关联的服务注册中心。
服务消费者直接通过调用被 @LoadBalanced 注解修饰过的 RestTemplate 来实现面向服务的接口调用。

#### Nginx 与 Ribbon 区别

Nginx 属于服务端负载均衡，Ribbon 属于客户端负载均衡。Nginx 作用与 Tomcat，Ribbon 作用与各个服务之间的调用 (RPC)。



## 7.接口调用 Feign

Feign 的关键机制是使用了动态代理。
RestTemplate 是 Spring 提供的一个访问 Http 服务的客户端类，微服务之间的调用是使用的 RestTemplate。Eureka 框架中的注册、续约等，底层都是使用的 RestTemplate。
每次都调用 RestRemplate 的 API 太麻烦，可以将被调用的服务代码映射到消费者端，就是 Open Feign。
Open Feign 也是运行在消费者端的，使用 Ribbon 进行负载均衡，所以 Open Feign 直接内置了 Ribbon。

Feign 是和 Ribbon 以及 Eureka 紧密协作的：
首先 Ribbon 会从 Eureka Client 里获取到对应的服务注册表，也就知道了所有的服务都部署在了哪些机器上，在监听哪些端口。
然后 Ribbon 就可以使用默认的 Round Robin 算法，从中选择一台机器。
Feign 就会针对这台机器，构造并发起请求。



## 8.网关 Zuul

Zuul 是从设备和 web 站点到 Netflix 流应用后端的所有请求的前门。作为边界服务应用，Zuul 是为了实现动态路由、监视、弹性和安全性而构建的。它还具有根据情况将请求路由到多个亚马逊自动缩放组（亚马逊的一种云计算方式）的能力。
网关是系统唯一对外的入口，介于客户端与服务器端之间，用于对请求进行鉴权、限流、 路由、监控等功能。
Zuul中最关键的功能：路由和过滤器。
Zuul 首先需要向 Eureka 进行注册，能拿到所有 Consumer 的信息如元数据(名称，ip，端口)来进行路由映射。对于路由规则的维护，Zuul 默认会将通过以服务名作为 ContextPath 的方式来创建路由映射。
过滤：能实现限流，灰度发布，权限控制等等。对微服务接口的拦截和校验。
Zuul 作为网关肯定也存在单点问题，如果我们要保证 Zuul 的高可用，我们就需要进行 Zuul 的集群配置，这个时候可以借助额外的一些负载均衡器比如 Nginx。

Zuul 网关也可以集成 Hystrix 实现网关层集中式限流容错。



## 9.配置管理 Config

既能对配置文件统一地进行管理，又能在项目运行时动态修改配置文件。
Spring Cloud Config 为分布式系统中的外部化配置提供服务器和客户端支持。使用 Config服务器，可以在中心位置管理所有环境中应用程序的外部属性。
Spring Cloud Config 就是能将各个应用/系统/模块的配置文件存放到统一的地方然后进行管理(Git 或者 SVN)。
启动的时候才会进行配置文件的加载，那么我们的 Spring Cloud Config 就暴露出一个接口给启动应用来获取它所想要的配置文件，应用获取到配置文件然后再进行它的初始化工作。
Webhooks，这是 github 提供的功能，它能确保远程库的配置文件更新后客户端中的配置信息也得到更新。
使用 Bus 消息总线 + Spring Cloud Config 进行配置的动态刷新。
Spring Cloud Bus：用于将服务和服务实例与分布式消息系统链接在一起的事件总线。在集群中传播状态更改很有用（例如配置更改事件）。
Spring Cloud Bus 的作用就是管理和广播分布式系统中的消息，也就是消息引擎系统中的广播模式。当然作为消息总线的 Spring Cloud Bus 可以做很多事而不仅仅是客户端的配置刷新功能。
拥有了 Spring Cloud Bus 之后，我们只需要创建一个简单的请求，并且加上 @ResfreshScope 注解就能进行配置的动态修改了。

Nacos、Apollo



## 10.微服务技术栈

#### 开发

Spring Boot、Spring Cloud：RESTful 框架，序列化协议主要采用基于文本的 JSON，通讯协议一般基于 HTTP。

#### 文档

Swagger。

#### 服务调用 

Rest、RPC、RMI

#### 服务熔断

Hystrix、Sentinel

#### 日志监控

ELK

#### 消息队列

Kafka、RocketMQ、RabbitMQ

#### 服务安全

OAuth

#### 权限认证

Apache Shiro、Spring Security

#### 服务部署

Docker、Kubernetes

#### 批处理

Spring Batch

#### 定时任务

Quartz

#### 服务监控

Zabbix、Nagios、Metrics

#### 服务链路追踪

Zipkin、Brave

#### 数据库